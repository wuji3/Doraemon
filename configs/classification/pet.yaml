model:
  task: classification # Task type: classification
  name: torchvision-swin_b # Model used: Swin Transformer (base version)
  image_size: &resize_size 224 # Input image size, using anchor for later reference
  kwargs: {} # Additional parameters passed to model initialization into torchvision-style models.__init__ 'attention_dropout': 0.1,
  num_classes: 35 # Number of classification categories
  pretrained: True # Whether to use pre-trained weights
  backbone_freeze: False # Whether to freeze the backbone network
  bn_freeze: False # Whether to freeze Batch Normalization layers
  bn_freeze_affine: False # Whether to freeze affine transformation parameters of BN layers
  attention_pool: False # Whether to use attention pooling
data:
  root: wuji3/oxford-iiit-pet # Dataset path or Hugging Face dataset name
  nw: 64 # if not multi-nw, set to 0
  train:
    bs: 320 # Batch size per GPU
    base_aug: null # Base data augmentation
    class_aug: null # Class-specific data augmentation
      # Abyssinian: [1,2,3,4]
      # Birman: [1,2,3,4]
    augment: # Data augmentation strategy, refer to utils/augment.py
       - random_choice:
          transforms: 
            - random_color_jitter:
                brightness: 0.1
                contrast: 0.1
                saturation: 0.1
                hue: 0.1
            - random_cutout:
                n_holes: 3
                length: 12
                prob: 0.1
                color: [0, 255]
            - random_gaussianblur:
                kernel_size: 5
            - random_rotate:
                degrees: 10
            - random_autocontrast:
                p: 0.5
            - random_adjustsharpness:
                p: 0.5
            - random_augmix: 
                severity: 3
       - random_horizonflip: 
           p: 0.5
       - random_choice:
          transforms:
            - resize_and_padding: 
                size: *resize_size
                training: True
            - random_crop_and_resize:
                size: *resize_size
                scale: [0.7, 1]
          p: [0.9, 0.1]
       - to_tensor: no_params
       - normalize: 
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
    aug_epoch: 13 # Number of epochs to apply data augmentation
  val:
    bs: 320
    augment:
        - resize_and_padding:
            size: *resize_size
            training: False
        - to_tensor: no_params
        - normalize: 
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
hyp:
  epochs: 15 # Total number of training epochs
  lr0: 0.06 # Initial learning rate
  lrf_ratio: null # Learning rate decay ratio, null means using default value 0.1
  momentum: 0.937 # Optimizer momentum
  weight_decay: 0.0005 # Weight decay
  warmup_momentum: 0.8 # Momentum during warmup phase
  warm_ep: 1 # Number of warmup epochs
  loss:
    ce: True # Whether to use cross-entropy loss
    bce: # Binary cross-entropy, config: [use or not, weight, multi-label or not]
      - False 
      - 0.5
      - False 
  label_smooth: 0.1 # Label smoothing coefficient
  strategy:
    prog_learn: False # Whether to use progressive learning
    mixup: # Mixup data augmentation configuration
      ratio: 0.0
      duration: 10
    focal: # Focal Loss, config: [use or not, alpha, gamma]
      - False
      - 0.25
      - 1.5
    ohem: # Online Hard Example Mining, config: [use or not, min_kept, thresh_prob, ignore_index]
      - False
      - 8 
      - 0.7 
      - 255 
  optimizer: 
    - sgd # Optimizer type: SGD, Adam, or SAM
    - False # Whether to set different learning rates for different layers of the model
  scheduler: cosine_with_warm # Learning rate scheduler: cosine annealing with warmup