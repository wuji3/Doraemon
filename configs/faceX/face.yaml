model:
  task: face
  image_size: &imgsz 224
  load_from: null
  backbone:
    timm-swin_base_patch4_window7_224.ms_in22k_ft_in1k: # imgsz 224
    # timm-vit_base_patch8_224.dino: # imgsz 224
    # timm-vit_large_patch16_224.mae: # imgsz 224
    # timm-vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k: # imgsz 224
    # timm-swinv2_large_window12to16_192to256.ms_in22k_ft_in1k: # imgsz 256
    # timm-vit_base_patch16_clip_224.laion2b_ft_in1k: # imgsz 224
    # timm-swinv2_cr_small_ns_224.sw_in1k: # imgsz 224
    # timm-vit_so400m_patch14_siglip_224.webli: # imgsz 224
    # timm-wide_resnet101_2.tv2_in1k: # imgsz 224
    # timm-tf_mobilenetv3_large_minimal_100.in1k: # imgsz 224
    # timm-vit_large_patch14_dinov2.lvd142m: # imgsz 518
      pretrained: True
      image_size: *imgsz
      feat_dim: &featd 128 
  head:
    arcface:
      feat_dim: *featd
      num_class: 58671
      margin_arc: 0.35
      margin_am: 0.0
      scale: 32
data:
  root: /home/duke/facedata
  nw: 64 # if not multi-nw, set to 0
  train:
    bs: 160 # per gpu
    common_aug: null 
    class_aug: null
    augment: # refer to utils/augment.py
       - random_choice:
          transforms: 
            - random_color_jitter:
                brightness: 0.1
                contrast: 0.1
                saturation: 0.1
                hue: 0.1
            - random_gaussianblur:
                kernel_size: 5
       - random_horizonflip:
           p: 0.5
       - random_choice:
          transforms:
            - resize_and_padding: 
                size: *imgsz
                training: True
            - random_crop_and_resize:
                size: *imgsz
                scale: [0.7, 1]
       - to_tensor: no_params
       - normalize: 
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
    aug_epoch: 28
  val:
    bs: 32
    pair_txt: <pairs.txt>
    augment:
        - resize_and_padding:
            size: *imgsz
            training: False
        - to_tensor: no_params
        - normalize: 
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
hyp:
  epochs: 30
  lr0: 0.006
  lrf_ratio: null # decay to lrf_ratio * lr0, if None, 0.1
  momentum: 0.937
  weight_decay: 0.0005
  warmup_momentum: 0.8
  warm_ep: 1
  loss:
    ce: True
  label_smooth: 0.0
  optimizer: 
    - sgd # sgd, adam or sam
    - False # Different layers in the model set different learning rates, in built/layer_optimizer
  scheduler: cosine_with_warm # linear or cosine
