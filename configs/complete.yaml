model:
  choice: torchvision-resnet18 # torchvision- or custom-
  kwargs: {} # kwargs feed into torchvision-style models.__init__
  num_classes: 6 # out_channels of fc, maybe be not equal to num_classes of train_dir. eg: maybe more than num_classes
  pretrained: True
  backbone_freeze: False
  bn_freeze: False
  bn_freeze_affine: False
data:
  root: ./data # -train -val
  nw: 0 # if not multi-nw, set to 0
  imgsz:
    - 640
    - 640 # [h,w] / [s,s] / [s,]->[adaptive, s] or [s, adaptive]
  train:
    bs: 32 # all gpus if DDP
    augment: # refer to utils/augment.py
      random_color_jitter:
        prob: 0.5
        brightness: 0.1
        contrast: 0.1
        saturation: 0.1
        hue: 0.1
      resize:
        size:
          - 640
          - 640
      to_tensor: no_params
      normalize: # default use ImageNet1K mean & var, if not pretrained, del normalize
        mean: (0.485, 0.456, 0.406)
        std: (0.229, 0.224, 0.225)
    aug_epoch: 0 # augment for epochs, on which epoch to weaken, except warm_epoch
  val:
    bs: 1
    augment:
      resize:
        size:
          - 640
          - 640
      to_tensor: no_params
      normalize: # default use ImageNet1K mean & var, if not pretrained, del normalize
        mean: (0.485, 0.456, 0.406)
        std: (0.229, 0.224, 0.225)
hyp:
  epochs: 5
  lr0: 0.001 # sgd=1e-2, adam=1e-3
  lrf_ratio: None # decay to lrf_ratio * lr0, if None, 0.1
  momentum: 0.937
  weight_decay: 0.0005
  warmup_momentum: 0.8
  warm_ep: 1 # out of epochs, imp linear
  loss:
    ce: True
    bce: False
  label_smooth: 0.1
  strategy:
    prog_learn: False # progressive learning, will effect on mixup and imgsz, devide epoch into 3 parts in default, mixup alpha 0 -> 0.1 -> 0.2, imgsz 0.5 -> ? -> 1
    mixup: 0 [0,4] # ratio and [start, end), on which epoch to start or delete mixup, support ce and bce, default alpha=0.1
    focal: False 0 # turn-on-or-off and start_epo (except warm_epoch)
    ohem:
      - True
      - 8 # min_kept
      - 0.7 # thresh_prob
      - 255 # ignore_index
  optimizer: sgd # sgd or adam
  scheduler: cosine_with_warm # linear or cosine